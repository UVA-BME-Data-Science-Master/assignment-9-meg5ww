---
title: "BME4550_Fall2018_Assignment2"
author: "Monika Grabowska"
date: "September 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(nycflights13)
```

## 5.2.4 Exercises

###1. Find all flights that 1) Had an arrival delay of two or more hours, 2) Flew to Houston (IAH or HOU), 3) Were operated by United, American, or Delta, 4) Departed in summer (July, August, and September), 5) Arrived more than two hours late, but didn’t leave late, 6) Were delayed by at least an hour, but made up over 30 minutes in flight, 7) Departed between midnight and 6am (inclusive)

# 1)
```{r}
filter(flights, arr_delay >= 120)
```

# 2)
```{r}
filter(flights, dest == "IAH" | dest == "HOU")
```

# 3)
```{r}
filter(flights, carrier == "UA" | carrier == "AA" | carrier == "DL")
```

# 4)
```{r}
filter(flights, month >= 7, month <= 9)
```

# 5)
```{r}
filter(flights, arr_delay >= 120, dep_delay <= 0)
```

# 6) 
```{r}
filter(flights, dep_delay >= 60, dep_delay - arr_delay >= 30)
```

# 7) 
```{r}
filter(flights, dep_time >= 0, dep_time <= 600)
```

###2. Another useful dplyr filtering helper is `between()`. What does it do? Can you use it to simplify the code needed to answer the previous challenges?

`between()` checks if values in a numeric vector fall between a specified range. 


Using `between()` in previous challenges:
```{r}
filter(flights, between(month, 7, 9))
```

```{r}
filter(flights, between(dep_time, 0, 600))
```

###3. How many flights have a missing `dep_time`? What other variables are missing? What might these rows represent?

```{r}
filter(flights, is.na(dep_time))
```

8,255 flights have a missing `dep_time`. These flights are also missing `dep_delay`, `arr_time`, and `arr_delay`, suggesting that these flights did not fly anywhere and thus likely represent canceled flights. 

###4. Why is `NA ^ 0` not missing? Why is `NA | TRUE` not missing? Why is `FALSE & NA` not missing? Can you figure out the general rule? (`NA * 0` is a tricky counterexample!)

```{r}
NA ^ 0
NA | TRUE
FALSE & NA
NA * 0 
```

`NA ^ 0` is not missing because any value to the 0th power equals 1. `NA | TRUE` is not missing because the second condition is `TRUE`, thus the result is `TRUE` (or logic - doesn't matter what the other condition is). `FALSE & NA` is not missing because the first condition is `FALSE`, thus the result is `FALSE` (and logic - `FALSE` and anything will always be `FALSE`). However, `NA * 0` is `NA`. 

The general rule is that if the value of the missing value could potentially change the outcome of the operation, the result is missing (for example, with `NA * 0`, for most values of the missing value, the result would be 0; however, if the missing value was `Inf`, then the result would be `NaN`).

## 5.3.1 Exercises

###1. How could you use `arrange()` to sort all missing values to the start? (Hint: use `is.na()`).

```{r}
arrange(flights, desc(is.na(dep_time)))
```

###2. Sort `flights` to find the most delayed flights. Find the flights that left earliest.

Most delayed flights:
```{r}
arrange(flights, desc(dep_delay))
```

Flights that left earliest: 
```{r}
arrange(flights, dep_delay)
```

###3. Sort `flights` to find the fastest flights.

```{r}
arrange(flights, desc(distance / air_time))
```

###4. Which flights travelled the longest? Which travelled the shortest?

Traveled longest: 
```{r}
arrange(flights, desc(distance))
```

Traveled shortest:
```{r}
arrange(flights, distance)
```

## 5.4.1 Exercises

###1. Brainstorm as many ways as possible to select `dep_time`, `dep_delay`, `arr_time`, and `arr_delay` from `flights`.

# 1)
```{r}
select(flights, dep_time, dep_delay, arr_time, arr_delay)
```

# 2)
```{r}
select(flights, starts_with("dep"), starts_with("arr"))
```

# 3)
```{r}
select(flights, 4, 6, 7, 9)
```

###2. What happens if you include the name of a variable multiple times in a `select()` call?

```{r}
select(flights, air_time, air_time)
```

Even if you include the name of a variable multiple times in a `select()` call, the variable is only included once in the data frame. 

###3. What does the `one_of()` function do? Why might it be helpful in conjunction with this vector?

```{r}
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
```

The `one_of()` function enables you to select variables matching elements in a character vector. 

For example, you could select the variables in `vars` from `flights`:

```{r}
select(flights, one_of(vars))
```

###4. Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?

```{r}
select(flights, contains("TIME"))
```

The select helpers ignore case by default. You can change that default by setting `ignore.case = FALSE`, as shown below. 

```{r}
select(flights, contains("TIME", ignore.case = 'FALSE'))
```

## 5.5.2 Exercises

###1. Currently `dep_time` and `sched_dep_time` are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.

```{r}
mutate(flights, dep_time_min = ((dep_time %/% 100 * 60) + (dep_time %% 100)) %% 1440,
  sched_dep_time_min = ((sched_dep_time %/% 100 * 60) + (sched_dep_time %% 100)) %% 1440)
```
(Need to have `%% 1440` at the end or otherwise midnight (2400) will be 24 * 60 = 1440, rather than 0)

###2. Compare `air_time` with `arr_time - dep_time`. What do you expect to see? What do you see? What do you need to do to fix it?

```{r}
f1 <- mutate(flights, diff = arr_time - dep_time)
select(f1, air_time, diff)
```

I would expect `air_time` and `arr_time - dep_time` to be the same, but that is not the case. This is because while `air_time` is in minutes, `arr_time` and `dep_time` are in HHMM or HMM format. To fix the problem, `arr_time` and `dep_time` need to be converted into continuous numbers. 

###3. Compare `dep_time`, `sched_dep_time`, and `dep_delay`. How would you expect those three numbers to be related?

```{r}
select(flights, dep_time, sched_dep_time, dep_delay)
```

I would expect `dep_time` to be equal to `sched_dep_time + dep_delay` (but `dep_time` and `sched_dep_time` first need to be converted into continuous numbers since `dep_delay` is in minutes).

###4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for `min_rank()`.

```{r}
filter(flights, min_rank(desc(dep_delay)) <= 10)
```

`min_rank()` assigns ties to the lowest rank.

###5. What does `1:3 + 1:10` return? Why?

```{r}
1:3 + 1:10
```

Since the length of the longer vector is not a multiple of the length of the shorter vector, the entirety of the shorter vector is added to the first three elements of the longer vector, the fourth through sixth elements of the longer vector, and the seventh through ninth elements of the longer vector, but then only the first element of the shorter vector is added to the last element of the longer vector. 

###6. What trigonometric functions does R provide?

Cosine, sine, tangent, arc-cosine, arc-sine, arc-tangent, and the two-argument arc-tangent.

## 5.6.7 Exercises

###1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: 1) A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. 2) A flight is always 10 minutes late. 3) A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 4) 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay?

```{r}
(delay_chars <- flights %>%
  group_by(flight) %>%
  summarize(fifteen_min_early = mean(arr_delay == -15, na.rm = TRUE), 
            fifteen_min_late = mean(arr_delay == 15, na.rm = TRUE),
            ten_min_late = mean(arr_delay == 10, na.rm = TRUE), 
            thirty_min_early = mean(arr_delay == -30, na.rm = TRUE),
            thirty_min_late = mean(arr_delay == 30, na.rm = TRUE),
            on_time = mean(arr_delay == 0, na.rm = TRUE),
            two_hr_late = mean(arr_delay == 120, na.rm = TRUE)))
```

#1)
```{r}
filter(delay_chars, fifteen_min_early == 0.5, fifteen_min_late == 0.5)
```

#2)
```{r}
filter(delay_chars, ten_min_late == 1)
```

#3)
```{r}
filter(delay_chars, thirty_min_early == 0.5, thirty_min_late == 0.5)
```

#4)
```{r}
filter(delay_chars, on_time == 0.99, two_hr_late == 0.01)
```

Arrival delay is more important, since it can directly affect connecting flights (with departure delay, there is a chance that time can be made up in the air). 

###2. Come up with another approach that will give you the same output as `not_cancelled %>% count(dest)` and `not_cancelled %>% count(tailnum, wt = distance)` (without using `count()`).

``` {r}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

not_cancelled %>% 
  count(dest)

not_cancelled %>% 
  count(tailnum, wt = distance)
```

Other approach:
```{r}
not_cancelled %>% 
  group_by(dest) %>%
  summarize(n = n())

not_cancelled %>% 
  group_by(tailnum) %>%
  summarize(wt = sum(distance))
```

###3. Our definition of cancelled flights (`is.na(dep_delay) | is.na(arr_delay)`) is slightly suboptimal. Why? Which is the most important column?

A flight cannot arrive without departing, this `dep_delay` appears to be the most important column and we can define cancelled flights by `is.na(dep_delay)`. 

###4. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?

```{r}
flights %>%
  mutate(cancelled = (is.na(dep_delay))) %>%
  group_by(year, month, day) %>%
  summarise(cancelled_prop = sum(cancelled) / n(), 
            avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(aes(avg_delay, cancelled_prop)) +
  geom_point() +
  geom_smooth()
```

There is a positive correlation between the proportion of cancelled flights and the average departure delay.

###5. Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about `flights %>% group_by(carrier, dest) %>% summarise(n())`)

```{r}
flights %>%
  group_by(carrier) %>%
  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  arrange(desc(avg_delay))
```

Carrier `F9` has the worst delays.

Challenge:
```{r}
flights %>% 
  group_by(carrier, dest) %>%
  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  group_by(carrier) %>%
  summarize(carrier_spread = mad(avg_delay, na.rm = TRUE)) %>%
  arrange(desc(carrier_spread))
```

By grouping by carrier and calculating the median absolute deviation of the mean departure delay for each carrier/destination pair, we can see which carriers experience a large amount of variation in the mean departure delays across destinations (i.e. which carriers have a high `carrier_spread`). Since these carriers are not consistently bad across destinations, this would indicate that bad airports are likely to blame. 

###6. What does the `sort` argument to `count()` do. When might you use it?

If `TRUE`, the `sort` argument to `count()` will sort output in descending order of n. You could use it instead of `count()` then `arrange()`. 

## 5.7.1 Exercises

###1. Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping.

When you combine functions like `mean()`, `sum()`, `median()`, etc... with grouping, the function will be applied to each group rather than to the whole dataset. 

###2. Which plane (`tailnum`) has the worst on-time record?

```{r}
flights %>% 
  group_by(tailnum) %>%
  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  arrange(desc(avg_delay))
```

Plane N844MH has the worst on-time record.

###3. What time of day should you fly if you want to avoid delays as much as possible?

```{r}
flights %>% 
  group_by(hour) %>%
  summarize(avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  arrange(avg_delay)
```

You should fly at 5 am if you want to avoid delays as much as possible.

###4. For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.

```{r}
flights %>% 
  group_by(dest) %>%
  summarize(total_delay_dest = sum(arr_delay, na.rm = TRUE))
```

```{r}
flights %>% 
  group_by(dest) %>%
  mutate(total_delay_dest = sum(arr_delay, na.rm = TRUE),
         prop_total_delay = arr_delay / total_delay_dest) %>%
  select(dest, arr_delay, total_delay_dest, prop_total_delay)
```

###5. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using `lag()`, explore how the delay of a flight is related to the delay of the immediately preceding flight.

```{r}
flights %>% 
  filter(!is.na(dep_delay)) %>% 
  group_by(dest) %>% 
  mutate(prev = lag(dep_delay)) %>%
  ggplot(mapping = aes(dep_delay, prev)) +
  geom_point() +
  geom_smooth()
```

###6. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?

```{r}
flights %>%
  group_by(dest) %>%
  arrange(air_time)
```

```{r}
flights %>%
  group_by(dest) %>%
  mutate(rel_air_time = air_time - min(air_time, na.rm = TRUE), na.rm = TRUE) %>%
  select(dest, air_time, rel_air_time, tailnum) %>%
  arrange(desc(rel_air_time))
```

Flight  N703TW is most delayed in air.

###7. Find all destinations that are flown by at least two carriers. Use that information to rank the carriers.

```{r}
flights %>%
  group_by(dest) %>%
  filter(n_distinct(carrier) >= 2) %>%
  group_by(carrier) %>%
  summarize(num_dest = n_distinct(dest)) %>%
  arrange(desc(num_dest)) 
```

###8. For each plane, count the number of flights before the first delay of greater than 1 hour.

```{r}
flights %>% 
  filter(!is.na(dep_delay)) %>% 
  group_by(tailnum) %>%
  mutate(delay_long = dep_delay > 60,
         delays_before = cumsum(delay_long)) %>%
  filter(delays_before < 1) %>%
  count(sort = TRUE)
```

## 7.3.4 Exercises

###1. Explore the distribution of each of the `x`, `y`, and `z` variables in `diamonds`. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x), binwidth = 0.5)
```

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(y), binwidth = 0.5)
```

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(z), binwidth = 0.5)
```

The distribution of the `x` variable is between 3.5 and 9 for the most part, while the distribution of the `y` variable is generally between 5 and 10, and the distribution of the `z` variable is generally between 2.5 and 5 (although there are a few outliers in the `y` and `z` variables). The `x` and `y` variables are likely length and width since they have similar distribution domains, while the `z` variable is likely depth. 

###2. Explore the distribution of `price`. Do you discover anything unusual or surprising? (Hint: Carefully think about the `binwidth` and make sure you try a wide range of values.)

Default binwidth:
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price))
```

Binwidth = 50:
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price), binwidth = 50)
```

Binwidth = 100:
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price), binwidth = 100)
```

Zoom in on binwidth = 100 plot: 
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price), binwidth = 100) +
  xlim(c(0,2500))
```

Suprisingly, there are relatively few diamonds priced around $1500. 

###3. How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?

```{r}
filter(diamonds, carat == 0.99) %>%
  count()
```

```{r}
filter(diamonds, carat == 1) %>% 
  count()
```

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(carat), binwidth = 0.01) +
  coord_cartesian(xlim = c(.98, 1.01))
```

There are 23 0.99 carat diamonds and 1558 1 carat diamonds. This is likely because even though 0.99 and 1 are very close, a 1 carat diamond seems more valuale/desirable since it is a full carat rather than a fraction of a carat. 

###4. Compare and contrast `coord_cartesian()` vs `xlim()` or `ylim()` when zooming in on a histogram. What happens if you leave `binwidth` unset? What happens if you try and zoom so only half a bar shows?

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price))
```

`coord_cartesian()`:
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price)) + 
  coord_cartesian(ylim = c(0, 4000))
```

`x_lim()`:
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(price)) +
  ylim(0, 4000)
```

`coord_cartesian()` draws the histogram first and then zooms in on the specified area, while `xlim()` and `ylim()` draw the histogram after dropping any values outside of the specified limits. 

## 7.4.1 Exercises

###1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?

```{r}
ggplot(flights, aes(dep_delay)) +
  geom_histogram()
```

```{r}
flights %>%
  mutate(carrier = ifelse(carrier == "UA", NA, carrier)) %>%
  ggplot(aes(carrier)) +
  geom_bar()
```

Missing values in a histogram are removed before drawing the histogram/calculating counts for each bin. Missing values in a bar chart are placed in a separate `NA` category. There is a difference because histograms are used to visualize continuous variables, while bar charts are used to visualize categorical variables. In a histogram, the variable on the x axis needs to have a numeric value. Since `NA` values do not have a numeric value, they cannot be placed in any bin and need to be removed. In a bar chart, on the other hand, the value on the x axis does not need to be numeric - the `NA` values can simply be counted together as another category. 

###2. What does `na.rm = TRUE` do in `mean()` and `sum()`?

It removes the missing values before computing `mean()` and `sum()`. 

## 7.5.1.1 Exercises

###1. Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights.

```{r}
flights %>%
  mutate(cancelled = is.na(dep_time), 
         sched_dep_time_min = ((sched_dep_time %/% 100 * 60) + (sched_dep_time %% 100)) %% 1440) %>%
  ggplot(mapping = aes(x = sched_dep_time_min, y = ..density..)) + 
  geom_freqpoly(mapping = aes(color = cancelled), binwidth = 25)
```

###2. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?

```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_point() +
  geom_smooth()
```

Carat size is most important for predicting the price of a diamond.

```{r}
ggplot(diamonds, aes(cut, carat)) +
  geom_boxplot()
```

Fair and good cut diamonds generally have a larger carat size than very good and ideal diamonds, which could explain why lower quality diamonds are more expensive.  

###3. Install the ggstance package, and create a horizontal boxplot. How does this compare to using `coord_flip()`?

```{r}
library(ggstance) 
ggplot(data = diamonds, mapping = aes(carat, cut)) + 
  geom_boxploth()
```

To get the same plot with `coord_flip`:
```{r}
ggplot(data = diamonds, mapping = aes(cut, carat)) + 
  geom_boxplot() +
  coord_flip()
```

###4. One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using `geom_lv()` to display the distribution of price vs cut. What do you learn? How do you interpret the plots?

With boxplot:
```{r}
ggplot(diamonds, aes(cut, price)) +
  geom_boxplot()
```

With lvplot:
```{r, error = TRUE}
library(lvplot)
ggplot(diamonds, aes(cut, price)) +
  geom_lv()
```

There are less outliers in the letter value plot than in the boxplot. The letter value plot also shows more quantiles than the boxplot (i.e. beyond the quartiles), which is useful for large datasets. 

###5. Compare and contrast `geom_violin()` with a facetted `geom_histogram()`, or a coloured `geom_freqpoly()`. What are the pros and cons of each method? 

`geom_violin()`:
```{r}
ggplot(diamonds, aes(cut, price)) +
  geom_violin()
```

Faceted `geom_histogram()`:
```{r}
ggplot(diamonds, aes(price)) +
  geom_histogram() +
  facet_grid(. ~ cut)
```

Colored `geom_freqpoly()`:
```{r}
ggplot(diamonds, aes(price, color = cut)) +
  geom_freqpoly()
```

`geom-violin()` and faceted `geom_histogram()` are useful for visualizing individual distributions, while colored `geom_freqpoly()` is useful for comparing distributions. 

###6. If you have a small dataset, it’s sometimes useful to use `geom_jitter()` to see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to `geom_jitter()`. List them and briefly describe what each one does.

`geom_quasirandom()` and `geom_beeswarm()` both are used to reduce overplotting by plotting points that would ordinarily overlap next to each other. `geom_quasirandom()` spaces the points based on either a van der Corput sequence or Tukey texturing; `geom_beeswarm()` uses a point-size based offset. 

## 7.5.2.1 Exercises

###1. How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut?

Distribution of cut within color:
```{r}
diamonds %>% 
  count(color, cut) %>% 
  group_by(color) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(mapping = aes(color, cut)) +
  geom_tile(mapping = aes(fill = prop))
```

Distribution of color within cut:
```{r}
diamonds %>% 
  count(color, cut) %>% 
  group_by(cut) %>% 
  mutate(prop = n / sum(n)) %>% 
  ggplot(mapping = aes(color, cut)) +
  geom_tile(mapping = aes(fill = prop))
```

###2. Use `geom_tile()` together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?

```{r}
flights %>% 
  group_by(dest, month) %>%
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(mapping = aes(month, dest)) +
  geom_tile(mapping = aes(fill = avg_dep_delay))
```

The plot is difficult to read since it is unordered, but this could be improved by sorting destinations by the average flight delay, as shown below. It is also hard to differentiate between the shades of blue, so a different color scale would also help to make the plot easier to read. 

```{r}
flights %>%
  group_by(month, dest) %>% 
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ungroup() %>% 
  group_by(dest) %>% 
  mutate(num_month = n()) %>% 
  ggplot(mapping = aes(factor(month), reorder(dest, num_month))) + 
  geom_tile(mapping = aes(fill = avg_dep_delay)) +
  scale_fill_distiller(type = "div", palette = "Spectral")
```

###3. Why is it slightly better to use `aes(x = color, y = cut)` rather than `aes(x = cut, y = color)` in the example above?

Using `aes(x = color, y = cut)`:
```{r}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = n))
```

Using `aes(x = cut, y = color)`:
```{r}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = cut, y = color)) +
  geom_tile(mapping = aes(fill = n))
```

It is better to use `aes(x = color, y = cut)` as this will produce approximately square tiles that are easier to compare to each other (compared to the rectangles that `aes(x = cut, y = color)` will produce). 

## 7.5.3.1 Exercises

###1. Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using `cut_width()` vs `cut_number()`? How does that impact a visualisation of the 2d distribution of `carat` and `price`?

```{r}
diamonds %>% 
  ggplot() +
  geom_freqpoly(mapping = aes(x = price, color = cut_width(carat, .25)))
```

```{r}
diamonds %>% 
  ggplot() +
  geom_freqpoly(mapping = aes(x = price, color = cut_number(carat, 10)))
```

Since `cut_width()` and `cut_number()` split a variable into groups, the number or width of bins chosen needs to be large enough to be able to visualize changes in distribution between groups but not too large. 

###2. Visualise the distribution of carat, partitioned by price.

```{r}
library(ggstance) 
ggplot(data = diamonds, mapping = aes(x = price, y = cut_number(carat, 10))) + 
  geom_boxploth()
```

###3. How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?

The price distribution for larger diamonds is more spread out. This is expected, as other factors such as color, cut, etc... may alter the pricing of large diamonds (these factors are less noticeable with smaller diamonds). 

###4. Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price.

```{r}
diamonds %>% 
  ggplot() +
  geom_boxplot(mapping = aes(x = cut, y = price, color = cut_number(carat, 5)))
```

```{r}
diamonds %>% 
  mutate(carat_group = cut_number(carat, 10)) %>%
  group_by(cut, carat_group) %>%
  summarize(avg_price = mean(price)) %>%
  ggplot() +
  geom_tile(mapping = aes(x = cut, y = carat_group, fill = avg_price)) +
  scale_fill_distiller(type = "div", palette = "Spectral")
```

###5. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of `x` and `y` values, which makes the points outliers even though their `x` and `y` values appear normal when examined separately. Why is a scatterplot a better display than a binned plot for this case?

Scatterplot: 
```{r}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))
```

Binned plot: 
```{r}
ggplot(data = diamonds) +
  geom_bin2d(mapping = aes(x = x, y = y), bins = 100) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))
```

In the scatterplot, we can see the outliers as individual points, rather than as binned counts, making the scatterplot a better display than a binned plot for this case. 