---
title: "BME4550_Fall2018_Assignment5"
author: "Monika Grabowska"
date: "October 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(gapminder)
```

## 23.2.1 Exercises

###1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```

```{r}
fit <- lm(y~x, data = sim1a)
ggplot(sim1a,aes(x,y))+
  geom_point(size = 2, color = "grey30")+
  geom_abline(intercept = fit$coefficients[1],slope = fit$coefficients[2])
```

An abnormal value can cause the fitted line to deviate markedly from the line that would be expected if that point were excluded. 

###2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance. Use `optim()` to fit this model to the simulated data above and compare it to the linear model.

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

best <- optim(c(0,0), measure_distance, data = sim1a)

best$par
```

###3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

best <- optim(par=c(0, 0, 0), fn=measure_distance, data = sim1a)
best1 <- optim(par=c(0, 0, 1), fn=measure_distance, data = sim1a)
best2 <- optim(par=c(0, 0, 2), fn=measure_distance, data = sim1a)

best$par
best1$par
best2$par
```

Depending on the starting point, can find different optimal values for the model. 

## 23.3.3 Exercises

###1. Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`. How does the result compare to `geom_smooth()`?

```{r}
fit_loess <- loess(y~x, data = sim1)
fit_lm <- lm(y~x, data = sim1)

grid <- sim1 %>% 
  data_grid(x)

grid_loess <- grid %>% 
  add_predictions(fit_loess)
sim1_loess <- sim1 %>% 
  add_residuals(fit_loess)

grid_lm <- grid %>% 
  add_predictions(fit_lm)
sim1_lm <- sim1 %>% 
  add_residuals(fit_lm)

plot_fit_loess <- ggplot(sim1, aes(x, y)) +
  geom_point() +
  geom_line(aes(x, pred), grid_loess, colour = "blue") 

plot_fit_loess
```

```{r}
plot_fit_loess +
  geom_smooth(colour = "red") 
```

`geom_smooth()` uses `loess()` by default, thus the predictions using `loess()` are the same as those using `geom_smooth()`. 

###2. `add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. How do these three functions differ?

`add_predictions()` adds predictions from a single model at a time (in a column), while `gather_predictions()` adds predictions from multiple models by adding a column with the model name and stacking the results of the different models. `spread_predictions()` adds predictions from multiple models at a time by adding multiple columns with predictions from one model per column. 

```{r}
fit_lm <- lm(y ~ x, data = sim1)
fit_loess <- loess(y ~ x, data = sim1)

grid <- sim1 %>%
  data_grid(x)

grid %>%
  add_predictions(fit_lm, var = "pred_lm") %>%
  add_predictions(fit_loess, var = "pred_loess")

grid %>%
  gather_predictions(fit_lm, fit_loess)

grid %>%
  spread_predictions(fit_lm, fit_loess)
```

###3. What does `geom_ref_line()` do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

`geom_ref_line()` adds a reference line to the plot. It comes from the ggplot2 package. Displaying a reference line in plots showing residuals is important because it makes it easier to visualize the characteristics of the residuals.

###4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

Looking at a frequency polygon of the absolute residuals helps you see the prediction's overall quality. However, it will not tell you anything about the distribution of residuals with respect to 0. 

## 23.4.5 Exercises

###1. What happens if you repeat the analysis of `sim2` using a model without an intercept. What happens to the model equation? What happens to the predictions?

With intercept:
```{r}
mod2 <- lm(y ~ x, data = sim2)
mod2$coefficients

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)

ggplot(sim2, aes(x,y)) + 
  geom_point(aes(x,y)) +
  geom_point(aes(x,pred), grid, color = "red", size = 4)
```

Without intercept: 
```{r}
mod2 <- lm(y ~ x - 1, data = sim2)
mod2$coefficients

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)

ggplot(sim2, aes(x,y)) + 
  geom_point(aes(x,y)) +
  geom_point(aes(x,pred), grid, color = "red", size = 4)
```

The intercept term is taken out of the model equation. The predictions are the same without an intercept as with an intercept. 

###2. Use `model_matrix()` to explore the equations generated for the models I fit to `sim3` and `sim4`. Why is `*` a good shorthand for interaction?

```{r}
model_matrix(data = sim3, y ~ x1 + x2)
model_matrix(data = sim3, y ~ x1 * x2)

model_matrix(data = sim4, y ~ x1 + x2)
model_matrix(data = sim4, y ~ x1 * x2)
```

`*` is a good shorthand for interaction because an interaction between x1 and x2 includes terms for x1, x2, and their product. 

###3. Using the basic principles, convert the formulas in the following two models into functions. (Hint: start by converting the categorical variable into 0-1 variables.)

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

```{r}
model_matrix_mod1 <- function(.data) {
  mutate(.data,
         `x2b` = as.numeric(x2 == "b"),
         `x2c` = as.numeric(x2 == "c"),
         `x2d` = as.numeric(x2 == "d")) %>%
    select(x1, x2b, x2c, x2d)
}
model_matrix_mod1(sim3)

model_matrix_mod2 <- function(.data) {
  mutate(.data, 
         `x2b` = as.numeric(x2 == "b"),
         `x2c` = as.numeric(x2 == "c"),
         `x2d` = as.numeric(x2 == "d"),
         `x1:x2b` = x1 * x2b,
         `x1:x2c` = x1 * x2c,
         `x1:x2d` = x1 * x2d) %>%
    select(x1, x2b, x2c, x2d, `x1:x2b`, `x1:x2c`, `x1:x2d`)
}
model_matrix_mod2(sim3)
```

###4. For `sim4`, which of `mod1` and `mod2` is better? I think `mod2` does a slightly better job at removing patterns, but it’s pretty subtle. Can you come up with a plot to support my claim?

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)

sim4_mods <- gather_residuals(sim4, mod1, mod2)

ggplot(sim4_mods, aes(x = resid, colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug()

ggplot(sim4_mods, aes(x = abs(resid), colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug()
```

`mod2` might do a better job at removing patterns since it has smaller residuals in the tails of the distribution between 2.5 and 5. 

## 24.2.3 Exercises

###1. In the plot of `lcarat` vs. `lprice`, there are some bright vertical strips. What do they represent?

The bright vertical strips in the plot of `lcarat` vs. `lprice` represent areas of high counts of diamonds. 

###2. If `log(price) = a_0 + a_1 * log(carat)`, what does that say about the relationship between `price` and `carat`?

It means price = exp(a_0) * carat^(a_1).

###3. Extract the diamonds that have very high and very low residuals. Is there anything unusual about these diamonds? Are they particularly bad or good, or do you think these are pricing errors?

```{r}
diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))

mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)

diamonds2 <- diamonds2 %>%
  add_residuals(mod_diamond, "lresid")

resid_quants <- quantile(diamonds2$lresid)

filtered <- diamonds2 %>% 
  filter( 
    !((lresid < resid_quants[["25%"]]) & (lresid > resid_quants[["75%"]])) 
    )

ggplot(filtered, aes(cut, price)) + geom_boxplot()

ggplot(filtered, aes(color, price)) + geom_boxplot()

ggplot(filtered, aes(clarity, price)) + geom_boxplot()
```

There are probably pricing errors since diamonds with better clarity tend to have lower prices according to the plots.  

###4. Does the final model, `mod_diamond2`, do a good job of predicting diamond prices? Would you trust it to tell you how much to spend if you were buying a diamond?

```{r}
mod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)

diamonds2 %>% 
  add_predictions(mod_diamond2) %>%
  add_residuals(mod_diamond2) %>%
  summarise(sq_err = sqrt(mean(resid^2)),
            abs_err = mean(abs(resid)),
            p975_err = quantile(resid, 0.975),
            p025_err = quantile(resid, 0.025))
```

Yes - The average squared error is 2^0.192 = 1.142 and the average absolute error is 2^0.149 = 1.109, thus the error is around 10-15%.

## 24.3.5 Exercises

###1. Use your Google sleuthing skills to brainstorm why there were fewer than expected flights on Jan 20, May 26, and Sep 1. (Hint: they all have the same explanation.) How would these days generalise to another year?

Jan 20, May 26, and Sep 1 are all Sundays before Monday holidays (Martin Luther King Day, Memorial Day, and Labor Day). These days do not generalize to another year (the holidays may not fall on a Monday in a different year). 

###2. What do the three days with high positive residuals represent? How would these days generalise to another year?

```
daily %>% 
  top_n(3, resid)
#> # A tibble: 3 x 5
#>   date           n wday  resid term 
#>   <date>     <int> <ord> <dbl> <fct>
#> 1 2013-11-30   857 Sat   112.  fall 
#> 2 2013-12-01   987 Sun    95.5 fall 
#> 3 2013-12-28   814 Sat    69.4 fall
```

These days represent Saturdays or Sundays around Thanksgiving or Christmas. These are days when the need to travel is particularly high. 

###3. Create a new variable that splits the `wday` variable into terms, but only for Saturdays, i.e. it should have `Thurs`, `Fri`, but `Sat-summer`, `Sat-spring`, `Sat-fall`. How does this model compare with the model with every combination of `wday` and `term`?

```{r}
library(nycflights13)
library(lubridate)

daily <- flights %>%
  mutate(date = make_date(year, month, day)) %>%
  group_by(date) %>%
  summarise(n = n())

daily <- daily %>%
  mutate(wday = wday(date, label = TRUE))

term <- function(date) {
  cut(date,
    breaks = ymd(20130101, 20130605, 20130825, 20140101),
    labels = c("spring", "summer", "fall")
  )
}

daily <- daily %>%
  mutate(term = term(date))

mod1 <- lm(n ~ wday, data = daily)
mod2 <- lm(n ~ wday * term, data = daily)

daily <- daily %>%
  mutate(wday2 = 
         case_when(.$wday == "Sat" & .$term == "summer" ~ "Sat-summer",
         .$wday == "Sat" & .$ term == "fall" ~ "Sat-fall",
         .$wday == "Sat" & .$term == "spring" ~ "Sat-spring",
         TRUE ~ as.character(.$wday)))

mod3 <- lm(n ~ wday2, data = daily)

daily %>% 
  gather_residuals(sat_term = mod3, all_interact = mod2) %>% 
  ggplot(aes(date, resid, colour = model)) +
    geom_line(alpha = 0.75)
```

The model with "Sat-term" appears to have higher residuals in the fall and lower residuals in the spring compared with the model with every combination of `wday` and `term`.

###4. Create a new `wday` variable that combines the day of week, term (for Saturdays), and public holidays. What do the residuals of that model look like?

```{r}
daily <- daily %>%
  mutate(wday3 = 
         case_when(.$date %in% lubridate::ymd(c(20130101, # new years
                                        20130121, # mlk
                                        20130218, # presidents
                                        20130527, # memorial
                                        20130704, # independence
                                        20130902, # labor
                                        20131028, # columbus
                                        20131111, # veterans
                                        20131128, # thanksgiving
                                        20131225)) ~ "holiday",
           .$wday == "Sat" & .$term == "summer" ~ "Sat-summer",
           .$wday == "Sat" & .$ term == "fall" ~ "Sat-fall",
           .$wday == "Sat" & .$term == "spring" ~ "Sat-spring",
           TRUE ~ as.character(.$wday)))

mod5 <- lm(n ~ wday3, data = daily)

daily <- daily %>% 
  add_residuals(mod5, "resid")

daily %>% 
  slice(1:20) %>% 
  select(date, wday, resid)
```

###5. What happens if you fit a day of week effect that varies by month (i.e. `n ~ wday * month`)? Why is this not very helpful?

There will only be 4-5 samples per month (since there are only 4-5 repeated weekdays in a month), which is not particularly helpful.

###6. What would you expect the model `n ~ wday + ns(date, 5)` to look like? Knowing what you know about the data, why would you expect it to be not particularly effective?

I would expect the model `n ~ wday + ns(date, 5)` to have no clear trends - from what I've seen so far in the data, there is not really a weekday trend across months, but rather a trend around holidays, so the model would not be particularly effective. 

###7. We hypothesised that people leaving on Sundays are more likely to be business travellers who need to be somewhere on Monday. Explore that hypothesis by seeing how it breaks down based on distance and time: if it’s true, you’d expect to see more Sunday evening flights to places that are far away.

```{r}
flights %>% 
  mutate(date = make_date(year, month, day),
         wday = wday(date, label = TRUE)) %>%
  group_by(wday) %>%
  summarise(dist_mean =  mean(distance),
            dist_median = median(distance)) %>%
  ggplot(aes(y = dist_mean, x = wday)) +
  geom_point()
```

```{r}
flights %>% 
  mutate(date = make_date(year, month, day),
         wday = wday(date, label = TRUE)) %>%
  group_by(wday, hour) %>%
  summarise(dist_mean =  mean(distance),
            dist_median = median(distance)) %>%
  ggplot(aes(y = dist_mean, x = hour, colour = wday)) +
  geom_point() + 
  geom_line()
```

The hypothesis does not appear to be true.

###8. It’s a little frustrating that Sunday and Saturday are on separate ends of the plot. Write a small function to set the levels of the factor so that the week starts on Monday.

```{r}
monday_first <- function(x) {
  forcats::fct_relevel(x, levels(x)[-1])  
}

daily <- daily %>% 
  mutate(wday = wday(date, label = TRUE))

ggplot(daily, aes(monday_first(wday), n)) + 
  geom_boxplot() + 
  labs(x = "Day of Week", y = "Number of flights")
```

## 25.2.5 Exercises

###1. A linear trend seems to be slightly too simple for the overall trend. Can you do better with a quadratic polynomial? How can you interpret the coefficients of the quadratic? (Hint you might want to transform `year` so that it has mean zero.)

```{r}
modquad <- function(df){
  lm(data = df, lifeExp ~ poly(year,2))
}

quad <- gapminder %>%
  mutate(year = year - mean(year)) %>%
  group_by(country)%>%
  nest()%>%
  mutate(model = purrr::map(data,modquad))

quad <- quad %>%
  mutate(resids = map2(data, model, add_residuals))

unnest(quad,resids)

unnest(quad, resids) %>%
  ggplot(aes(group = country))+
  geom_line(aes(x = year, y = resid)) +
  facet_wrap(~continent, nrow = 2)

quad %>% 
  mutate(glance = map(model, broom::glance)) %>%
  unnest(glance, .drop = TRUE) %>%
  arrange(r.squared)
```

The `R^2` is better with a quadratic polynomial than with a linear trend. 

###2. Explore other methods for visualising the distribution of `R^2` per continent. You might want to try the ggbeeswarm package, which provides similar methods for avoiding overlaps as jitter, but uses deterministic methods.

```{r}
library(ggbeeswarm)

gapminder %>%
  mutate(year = year - mean(year)) %>%
  group_by(continent,country)%>%
  nest() %>%
  mutate(model = map(data, modquad)) %>%
  mutate(glance = map(model, broom::glance))%>%
  unnest(glance) %>%
  ggplot(aes(x= continent, y =r.squared,color = continent)) + 
  geom_beeswarm()
```

###3. To create the last plot (showing the data for the countries with the worst model fits), we needed two steps: we created a data frame with one row per country and then semi-joined it to the original dataset. It’s possible to avoid this join if we use `unnest()` instead of `unnest(.drop = TRUE)`. How?

```{r}
country_model <- function(df) {
  lm(lifeExp ~ poly(year,1), data = df)
}

by_country <- gapminder %>% 
  group_by(country, continent) %>% 
  nest()

by_country <- by_country %>% 
  mutate(model = purrr::map(data, country_model))

glance <- by_country %>% 
  mutate(glance = purrr::map(model, broom::glance)) %>% 
  unnest(glance)

glance %>% 
  filter(r.squared < 0.25) %>% 
  select(country,data) %>% 
  unnest() %>% 
  ggplot() +
  geom_line(aes(year,lifeExp,color=country))
```

## 25.4.5 Exercises

###1. List all the functions that you can think of that take an atomic vector and return a list.

One function that takes in an atomic vector and returns a list is `quantile`. 

###2. Brainstorm useful summary functions that, like `quantile()`, return multiple values.

Some summary functions that return multiple values are `range()`, `fivenum()`, etc...

###3. What’s missing in the following data frame? How does `quantile()` return that missing piece? Why isn’t that helpful here?

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise(q = list(quantile(mpg))) %>% 
  unnest()
```

The quantiles of the values are missing (i.e. 0%, 25%, 50%, etc...). `quantile()` returns them in the names of the vector, but `unnest()` drops the names of the vector, so those labels are lost. 

###4. What does this code do? Why might might it be useful?

```{r}
mtcars %>% 
  group_by(cyl) %>% 
  summarise_each(funs(list))
```

This code creates a data frame where each row corresponds to a value of `cyl`, and each observation for each column (other than the first column, which is `cyl`) is a list of all the values of that column for that particular value of cyl. It might be useful to have all the observations of each variable for each group.

## 25.5.3 Exercises

###1. Why might the `lengths()` function be useful for creating atomic vector columns from list-columns?

The `lengths()` function gets the length of each element in a list. It could be useful for creating atomic vector columns from list-columns since it can be used to test whether all elements in a list-column are the same length. 

###2. List the most common types of vector found in a data frame. What makes lists different?

The most common types of vector found in a data frame are character, integer, numeric, logical, and factor. Lists are different since they are non-atomic, which means they are a mixture of the aforementioned types. 

