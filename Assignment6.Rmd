---
title: "BME4550_Fall2018_Assignment6"
author: "Monika Grabowska"
date: "October 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(UsingR)
library(contrast)
library(dagdata)

```

## Matrix Algebra Exercises

###1. What is the average height of the sons (don’t round off)?

```{r}
data("father.son",package="UsingR")
mean(father.son$sheight)
```

The average height of the sons is 68.68407 inches.

###2. What is the mean of the son heights for fathers that have a height of 71 inches (don’t round off your answer)? Hint: use the function `round` on the fathers’ heights.

```{r}
mean(father.son$sheight[round(father.son$fheight)==71])
```

The mean of the son heights for fathers that have a height of 71 inches is 70.54082 inches.

###3. Which of the following cannot be written as a linear model?

Y = a + b^t + e cannot be written as a linear model. 

###4. Which of the following do you feel best describes what e represents?

Between individual variability: people of the same height vary in their weight.

----

###1. What is the entry in row 25, column 3?

```{r}
X = matrix(1:1000,100,10)
X[25,3]
```

The entry in row 25, column 3 is 225. 

###2. Using the function cbind, create a 10 x 5 matrix with first column x=1:10. Then add 2*x, 3*x, 4*x and 5*x to columns 2 through 5. What is the sum of the elements of the 7th row?

```{r}
x=1:10
mat <-cbind(x,2*x,3*x,4*x,5*x)
sum(mat[7,])
```

The sum of the elements of the 7th row is 105. 

###3. Which of the following creates a matrix with multiples of 3 in the third column?

```{r}
matrix(1:60,20,3,byrow=TRUE)
```

`matrix(1:60,20,3,byrow=TRUE)` creates a matrix with multiples of 3 in the third column.

----

###1. Suppose X is a matrix in R. Which of the following is not equivalent to X?

```{r}
X <- matrix(1:6,3,2)
X %*% matrix(1,ncol(X) )
```

`X %*% matrix(1,ncol(X) )` 

###2. Solve the following system of equations using R: 3a + 4b − 5c + d = 10, 2a + 2b + 2c − d = 5, a − b + 5c − 5d = 7, 5a + d = 4. What is the solution for c?

```{r}
x <- matrix(c(3,2,1,5,4,2,-1,0,-5,2,5,0,1,-1,-5,1),4,4)
y <- matrix(c(10,5,7,4),4,1)
solve(x,y)
```

c is -0.8849558. 

###3. Load the following two matrices into R: `a <- matrix(1:12, nrow=4)`, `b <- matrix(1:15, nrow=3)`. What is the value in the 3rd row and the 2nd column of the matrix product of `a` and `b`?

```{r}
a <- matrix(1:12, nrow=4)
b <- matrix(1:15, nrow=3)
c <- a %*% b
c[3,2]
```

The value in the 3rd row and the 2nd column of the matrix product of `a` and `b` is 113. 

###4. Multiply the 3rd row of `a` with the 2nd column of `b`, using the element-wise vector multiplication with *. What is the sum of the elements in the resulting vector?

```{r}
sum(a[3,] * b[,2])
```

The sum of the elements in the resulting vector is 113. 

----

###1. Suppose we are analyzing a set of 4 samples. The first two samples are from a treatment group A and the second two samples are from a treatment group B. This design can be represented with a model matrix like so:

```{r}
X <- matrix(c(1,1,1,1,0,0,1,1),nrow=4)
rownames(X) <- c("a","a","b","b")
X
```

###Suppose that the fitted parameters for a linear model give us: 

```{r}
beta <- c(5, 2)
```

###What is the fitted value for the A samples? 

```{r}

X.a <- X[rownames(X) == "a",]
X.a %*% beta
```

###2. What is the fitted value for the B samples?

```{r}
X.b <- X[rownames(X) == "b",]
X.b %*% beta
```

###3. Suppose now we are comparing two treatments B and C to a control group A, each with two samples. This design can be represented with a model matrix like so:

```{r}
X <- matrix(c(1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1),nrow=6)
rownames(X) <- c("a","a","b","b","c","c")
X
```

###Suppose that the fitted values for the linear model are given by:

```{r}
beta <- c(10,3,-3)
```

###What is the fitted value for the B samples?

```{r}
X.b = X[rownames(X) == "b",]
X.b %*% beta
```

###4. What is the fitted value for the C samples?

```{r}
X.c = X[rownames(X) == "c",]
X.c %*% beta
````

----

## Linear Models Exercises

###1. Given how we have defined A, which of the following is the LSE of g, the acceleration due to gravity? Hint: try the code in R.

```{r}
g <- 9.8
h0 <- 56.67
v0 <- 0
n <- 25
tt <- seq(0,3.4,len=n) 
y <- h0 + v0*tt - 0.5*g*tt^2 + rnorm(n,sd=1)

X = cbind(1,tt,tt^2)
A = solve(crossprod(X))%*%t(X)
A
```

```{r}
-2 * (A %*% y) [3]
```

`-2 * (A %*% y) [3]` is the LSE of g. 

###2. In the lines of code above, the function rnorm introduced randomness. This means that each time the lines of code above are repeated, the estimate of g will be different. Use the code above in conjunction with the function replicate to generate 100,000 Monte Carlo simulated datasets. For each dataset, compute an estimate of g. (Remember to multiply by -2.) What is the standard error of this estimate?

```{r}
betahat <- replicate(100000, {
  y <- h0 + v0*tt - 0.5*g*tt^2 + rnorm(n,sd=1)
  betahats <- -2 * (A %*% y) [3]
  return(betahats)
})
sd(betahat)
```

###3. In the father and son height examples, we have randomness because we have a random sample of father and son pairs. For the sake of illustration, let’s assume that this is the entire population:

```{r}
x = father.son$fheight
y = father.son$sheight
n = length(y)
```

###Now let’s run a Monte Carlo simulation in which we take a sample of size 50 over and over again. Here is how we obtain one sample:

```{r}
N = 50
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat = lm(y~x)$coef
```

###Use the function replicate to take 10,000 samples. What is the standard error of the slope estimate? That is, calculate the standard deviation of the estimate from the observed values obtained from many random samples.

```{r}
betahat <- replicate(10000, {
  index <- sample(n, 50)
  sampledat <- father.son[index,]
  x <- sampledat$fheight
  y <- sampledat$sheight
  return(lm(y ~ x)$coef[2])
})
sd(betahat)
```

###4. Which of the following is closest to the covariance between father heights and son heights?

```{r}
cov(father.son$fheight, father.son$sheight)
```

4 is closest to the covariance between father heights and son heights.

----

###1. Given the factors we have defined above and without defining any new ones, which of the following R formula will produce a design matrix (model matrix) that lets us analyze the effect of condition, controlling for the different days?

```{r}
condition <- factor(c("treated","treated","treated","treated","treated","treated","control","control","control","control","control","control"))
day<- factor(c("A","A","B","B","C","C","A","A","B","B","C","C"))

table(condition,day)

model.matrix(~ day + condition)
```

`~ day + condition` 

----

###1. You can make a design matrix X for a two group comparison, either using model.matrix or simply with: `X <- cbind(rep(1,Nx + Ny),rep(c(0,1),c(Nx, Ny)))`. In order to compare two groups, where the first group has Nx=5 samples and the second group has Ny=7 samples, what is the element in the 1st row and 1st column of X⊤X?

```{r}
nx=5
ny=7

X = cbind(rep(1,nx + ny),rep(c(0,1),c(nx, ny)))

XtX = t(X) %*% X
XtX[ 1,1 ]
```

The element in the 1st row and 1st column of X⊤X is 12. 

###2. The other entries of X⊤X are all the same. What is this number?

```{r}
t(X) %*% X
```

This number is 7. 

----

###1. In the previous assessment, we used a Monte Carlo technique to see that the linear model coefficients are random variables when the data is a random sample. Now we will use the previously seen matrix algebra to try to estimate the standard error of the linear model coefficients. Again, take a random sample of the father.son heights data:

```{r}
x = father.son$fheight
y = father.son$sheight

n = length(y)
N <- 50
set.seed(1)
index <- sample(n,N)
sampledat <- father.son[index,]
x <- sampledat$fheight
y <- sampledat$sheight
betahat <- lm(y~x)$coef
```

###The fitted values Yˆ from a linear model can be obtained with:

```{r}
fit <- lm(y ~ x)
fit$fitted.values
```

###What is the sum of the squared residuals?

```{r}
e <- y-fit$fitted.values
ssr <- sum(e^2)
```

The sum of the squared residuals is 256.2152.

###2. Use the answer from exercise 1 to provide an estimate of sigma squared.

```{r}
sigma_sq <- ssr / 48
sigma_sq
```

###3. Form the design matrix X (Note: use a capital X). This can be done by combining a column of 1’s with a column containg ‘x’ , the fathers’ heights.

```{r}
N <- 50
X <- cbind(rep(1,N), x)
```

###Now calculate (X⊤X)^−1. Use the `solve` function for the inverse and t for the transpose. What is the element in the first row, first column?

```{r}
X = cbind(rep(1,N), x) # form design matrix
el <- solve(t(X) %*% X) # calculate (X⊤X)^−1
el11 <- el[1,1]
el11
```

The element in the first row, first column is 11.30275.

###4. Now we are one step away from the standard error of betahat. What is the standard error for the slope?

```{r}
sqrt(diag(solve(t(X) %*% X)) * (sigma_sq))
```

The standard error for the slope is 0.1141966. 

###Compare your answer to this last question, to the value you estimated using Monte Carlo in the previous set of exercises. It will not be the same because we are only estimating the standard error given a particular sample of 50 (which we obtained with set.seed(1)).

```{r}
lm(formula = y ~ x)
```

The standard error for the slope in the previous set of exercises is 0.5857.

----

###Suppose we have an experiment with two species A and B, and two conditions, control and treated.

```{r}
species <- factor(c("A","A","B","B"))
condition <- factor(c("control","treated","control","treated"))
model.matrix(~ species + condition)
```

###1. What should the contrast vector be, to obtain the difference between the species B control group and the species A treatment group (species B control - species A treatment)? Assume that the coefficients (columns of design matrix) are: Intercept, speciesB, conditiontreated.

```{r}
y = rnorm(4)
fit = lm(y ~ species + condition)
contrast(fit, list(species="B",condition="control"), list(species="A",condition="treated"))$X
``` 

The contrast vector should be `0 1 -1`. 

###2. . Use the Rmd script to load the spider dataset. Suppose we build a model using two variables: `∼ type + leg.` What is the t-statistic for the contrast of leg pair L4 vs. leg pair L2?

```{r}
spider <- read.csv("spider_wolff_gorb_2013.csv", skip=1)

fitTL <- lm(friction ~ type + leg, data=spider)
summary(fitTL)
coefs <- coef(fitTL)
L4vsL2 <- contrast(fitTL, list(leg="L4",type="pull"),list(leg="L2",type="pull"))
L4vsL2$testStat
```

The t-statistic for the contrast of leg pair L4 vs. leg pair L2 is 2.451974.

###3. Using the estimate of the estimated covariance matrix, what is your estimate of cov(betahat_L4, betahat_L2)? 

```{r}
X <- model.matrix(~ type + leg, data=spider)
Sigma.hat <- sum(fitTL$residuals^2)/(nrow(X) - ncol(X)) * solve(t(X) %*% X)
C <- matrix(c(0,0,-1,0,1),1,5)

Sigma.hat[3,5]
```

My estimate of cov(betahat_L4, betahat_L2) is 0.0006389179. 

###4. Suppose that we notice that the within-group variances for the groups with smaller frictional coefficients are generally smaller, and so we try to apply a transformation to the frictional coefficients to make the within-group variances more constant. Add a new variable log2friction to the spider dataframe:

```{r}
spider$log2friction <- log2(spider$friction)
```

###The Y values now look like:

```{r}
boxplot(log2friction ~ type*leg, data=spider)
```

###Run a linear model of log2friction with type, leg and interactions between type and leg. What is the t-statistic for the interaction of type push and leg L4? If this t-statistic is sufficiently large, we would reject the null hypothesis that the push vs. pull effect on `log2(friction)` is the same in L4 as in L1.

```{r}
fitln <- lm(log2friction ~ type*leg, data=spider)
summary(fitln)
```

The t-statistic for the interaction of type push and leg L4 is -3.689. 

###5. Using the same analysis of log2 transformed data, What is the F-value for all of the type:leg interaction terms in an ANOVA? If this value is sufficiently large, we would reject the null hypothesis that the push vs. pull effect on `log2(friction)` is the same for all leg pairs.

```{r}
anova(fitln)
```

The F-value for all of the type:leg interaction terms in an ANOVA is 10.701. 

###6. What is the L2 vs. L1 estimate in `log2(friction)` for the pull samples?

```{r}
contrast(fitln, list(type="pull",leg="L2"), list(type="pull",leg="L1"))
```

The L2 vs. L1 estimate in `log2(friction)` for the pull samples is 0.3468125. 

###7. What is the L2 vs. L1 estimate in `log2(friction)` for the push samples? Remember, because of the interaction terms, this is not the same as the L2 vs L1 difference for the pull samples. If you’re not sure use the `contrast` function. Another hint: consider the arrows plot for the model with interactions.

```{r}
contrast(fitln, list(type="push",leg="L2"), list(type="push",leg="L1"))
```

The L2 vs. L1 estimate in `log2(friction)` for the push samples is 0.4464843.

----

###1. Which of the above design matrices does NOT have the problem of collinearity?

Only design matrix E does not have the problem of collinearity (i.e. the rank of E is equal to the number of columns, so the columns are independent).

```{r}
m = matrix(c(1,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1),4,4)
m
qr(m)$rank
```

###2. What is the sum of squared residuals when the male coefficient is 1 and the D coefficient is 2, and the other coefficients are fit using the linear model solution?

```{r}
sex <- factor(rep(c("female","male"),each=4))
trt <- factor(c("A","A","B","B","C","C","D","D"))
X <- model.matrix( ~ sex + trt)
Y <- 1:8

makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b

fitTheRest <- function(a,b) {
Ystar <- makeYstar(a,b)
Xrest <- X[,-c(2,5)]
betarest <- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar
residuals <- Ystar - Xrest %*% betarest
sum(residuals^2)
}
```

```{r}
fitTheRest(1,2)
```

The sum of squared residuals is 11. 

###3. We can run fitTheRest on a grid of values, using the following code (the Vectorize is necessary as outer requires only vectorized functions). In the grid of values, what is the smallest sum of squared residuals?

```{r}
outer(-2:8,-2:8,Vectorize(fitTheRest))
```

In the grid of values, the smallest sum of squared residuals is 2. 
